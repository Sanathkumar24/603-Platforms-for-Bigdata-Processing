# Assignment-1


## Q1. Big Data with example and types

>Big Data is a massive amount of information that is expanding exponentially over time. It is a data set that is so huge and complicated that typical data management systems cannot store or analyze it efficiently. Big data is a type of data that is extremely large.

- <strong>Example</strong>: Marketing and Advertising
>Advertisements have traditionally been targeted to various customer groups. In the past, marketers used TV and radio choices, survey results, and focus groups to predict people's reactions to advertisements. These tactics amounted to informed guessing at best. Advertisers now buy or collect massive amounts of data to determine what customers really click on, search for, and "like." Marketing efforts are now analyzed for efficiency using button rates, views, and other exact indicators. Amazon, for example, collects vast amounts of data on the orders, methods of delivery, and payment choices of its billions of customers. The firm then sells highly targeted ad slots to extremely precise groupings and subgroups.

### Types of Big data:
- <strong>Structured</strong>:  Structured data is defined as any information that can be collected, retrieved, and processed in a fixed manner.
- <strong>Unstructured</strong>: Unstructured data is defined as any data having an undetermined shape or structure. Aside from its massive quantity, unstructured data presents a number of issues in terms of processing and extracting value from it. A heterogeneous data source including a mix of basic text files, photos, movies, and so on is an example of unstructured data.
- <strong>Semi-Structured</strong>: Semi-structured data can include both types of information. We can think of semi-structured data as structured in form, but it is not.


#### Ref:  https://www.guru99.com/what-is-big-data.html




## Q2. 6 ‘V’ s of Big Data

### <strong>Volume</strong>

The primary element of big data is volume, which is primarily concerned with the connection between quantity and processing capability. This feature is rapidly changing as data collecting increases. Likewise, IT storage and processing capacity.
Example: A high-volume data collection would include all transactions made with credit card on a single day.

### <strong>Variety</strong>

The V of variety refers to the vast range of data that has been saved but has yet to be analyzed and examined. New data sources, such as social media and mobile devices, supplement established forms of structured information.
Example: CCTV audio and video recordings generated at many points around a city are an example of a high variety data set.

### <strong>Velocity</strong>

The pace at which data is created is referred to as its velocity. High velocity data is created at such a rapid rate that it requires the use of unique processing techniques.
Example: Twitter tweets or Facebook postings are examples of data that is created at a high rate.

### <strong>Value</strong>

The value which big data may offer, and it is closely related to what enterprises can accomplish with that data The ability to extract value through big data is required, since the value of big data rises considerably based on the information that can be obtained from it.

### <strong>Veracity</strong>

Veracity demonstrates the integrity and source of data, allowing it to be regarded doubtful, contradictory, or impure, and offers information on issues you are unsure how to handle. In a word, the data's validity, and reliability.
Data in volume can cause confusion, but less data can only transmit partial or incomplete information.

### <strong>Variability</strong>

Finally, variability: how much and how quickly is the architecture of your data modifying? How frequently does the content or form of your data change?
Example: A soda store may offer six distinct soda mixes, but if you receive the same soda blend daily that taste different, that is variety.


#### Ref:https://www.motivaction.nl/en/news/blog/big-data-the-6-vs-you-need-to-look-at-for-important-insights#:~:text=The%20various%20Vs%20of%20big,%2C%20value%2C%20veracity%20and%20variability.




## Q3.Phases of Big Data analysis

### 5 Phases of bigdata are:

### <strong>1. Data Integration</strong>

Data must initially be made accessible from the source systems in order to be processed in a datastore or data warehouse, analyzed, or made useable for other systems. Some of the example sources are SAP, Salesforce, and other internal systems, Web scraping data, Oracle, Microsoft, MySQL, and other internal databases.

### <strong>2. Data Storage</strong>

Essentially, this implies that Big Data demands different technology than traditional systems. Known technologies include HDFS, HBASE, Cassandra, MongoDB, Google Bigtable, and many more, as well as systems that combine big data with traditional systems, such as Amazon Redshift or Google BigQuery.

### <strong>3. Data Analytics</strong>

The usage of Big Data technology opens up new avenues for data use. Larger volumes of information can be done faster due to greater available processing power — keyword cloud.

#### Some of the examples:

- Deep Learning is based on enormous volumes of data, such as photographs.

- Machine Learning – more data implies a potentially better model explanation.

### <strong>4. Visualization</strong>

Data visualization techniques and tools are critical in the Big Data era for analyzing enormous volumes of information and making data-driven decisions, since data is being used for significant management decisions. As a result, there is a shift away from gut instinct and emotional decisions and toward rational decisions based on data. As a result, reports and visualizations must be simple to understand and relevant. It is becoming increasingly important for experts to be able to utilize data to make judgments and images to convey stories about how data informs the person, subject, time, location, and method questions. Because of the massive volumes of data, Big Data visualization presents new opportunities and problems. As a result, new visualization techniques were developed in order to make the data volumes more apparent to the viewer.

### <strong>5. Data Security</strong>

Data privacy and data protection are critical. Not just banks and major IT corporations have valuable user and customer data. Almost every business has data on

Information of customers 

Employee data 

Financial information

#### Ref https://towardsdatascience.com/the-five-stages-for-big-data-b89ad1e8e156



## Q4. Challenges in Big Data analysis

### <strong>1. Business analytics solution fails to provide new or timely insights:</strong>

Assume you've invested in an analytics system in the hopes of gaining valuable insights that will help you make better business decisions. However, it appears that the insights provided by your new system are of the same degree and quality as those you previously had. Depending on the primary reason, this problem can be tackled via either a business or a technological lens.

### <strong>2. Analytical errors:</strong>

- <strong>Source data of poor quality:</strong>

Poor outcomes will be obtained if your system is based on data that has flaws, inaccuracies, or is incomplete. Data quality management and a mandatory data validation procedure encompassing all stages of your ETL process may assist assure the integrity of data received at differing stages (syntactic, semantic, grammatical, business, etc.). It will allow you to discover and eliminate inaccuracies while also ensuring that a change in one area is promptly reflected across the board, resulting in pure and accurate data.

- <strong>System defects related to the data flow:</strong>

This occurs when system requirements are neglected or not fully satisfied because of human mistake participation in the testing, or regulatory factor. High-quality development lifecycle testing and verification decreases the amount of such issues, which in turn reduces data processing issues. Even when dealing with high-quality data, your analytics may produce erroneous findings.


### <strong>3. Using predictive analytics in complex situations:</strong>

The next issue may render all efforts to develop an effective solution futile. If data analytics grows overly intricate, it may be tough to earn profits from your data. The complexity problem is generally reduced to either the UX or technical elements. 

- <strong>Data visualization is a mess:</strong>

Your reports' degree of intricacy is excessive. It takes too long or is difficult to locate the relevant information. This may be remedied by hiring a UI/UX consultant, who can assist you in developing an appealing adaptable customer interface that is simple to browse and operate with.

- <strong>The system has been overengineered:</strong>
The system evaluates more scenarios and provides you with more information than you require, distorting the focus. This also uses up more physical hardware and raises your prices. Therefore, users only use a portion of the capability. The remainder hangs like dead weight, and the answer appears to be overly difficult.

### <strong>4. Long reaction time of the system:</strong>

The software takes too long to evaluate the data, despite the fact that the input is already accessible and the analysis is urgently required. It may not be as important in batch processing, but in real-time systems, such delays might be costly.

- Data organization that is inefficient

- Infrastructure and resource usage issues with big data analytics

### <strong>5. Maintenance is expensive:</strong>
Any system needs constant investment in its infrastructure and upkeep. As well as every company owner wants to keep these investments to a minimum. As a result, even if you are satisfied with the maintenance costs and infrastructure, it is always a worthwhile endeavor to take a new look at the system and ensure that you are not overspending.
obsolete technologies
Inadequate infrastructure
The system you selected is overengineered.


#### Ref: https://www.analyticsinsight.net/5-challenges-of-big-data-analytics-in-2021/
